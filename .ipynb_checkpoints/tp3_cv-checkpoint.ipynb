{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Práctico 3: Detectando barcos en el agua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Dropout, Convolution2D, MaxPooling2D, Flatten, Conv2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import timeit\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# configuraciones generales de las imagenes\n",
    "PICTURE_SIZE = 80\n",
    "CHANNELS = 'rgb' # canales\n",
    "PATH_IMAGE = 'imagenes_tp3_cv/' #ubicación de las imágenes\n",
    "PATH_PKL = 'Archivos_pkl'\n",
    "\n",
    "INPUT_COLUMNS = []\n",
    "OUTPUT_COLUMNS = ['Barco']\n",
    "DATASET_COLUMNS = []\n",
    "# Crea lista el nombre de las columas\n",
    "for color in CHANNELS:\n",
    "    INPUT_COLUMNS.extend(['%s%i' % (color, i) \n",
    "                              for i in range(PICTURE_SIZE ** 2)])\n",
    "    DATASET_COLUMNS.extend(['%s%i' % (color, i) \n",
    "                              for i in range(PICTURE_SIZE ** 2)])\n",
    "    \n",
    "\n",
    "DATASET_COLUMNS.append('Barco')\n",
    "DATASET_COLUMNS.append('ImageName')\n",
    "\n",
    "print(DATASET_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada fila es una imagen.\n",
    "La imagen tiene 80 x 80 pixeles, y cada pixel tiene 3 valores, entonces tenemos 80 x 80 x 3 = 19.200‬ columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Imagen')\n",
    "imagen = Image.open(PATH_IMAGE + '0__20160622_170157_0c64__-122.485753590087_37.835957669247584.png')\n",
    "imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cant_image = len(os.listdir(PATH_IMAGE)) \n",
    "print('Hay ' + str(cant_image) + ' imagenes en la carpeta ' + PATH_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#comienza a correr el tiempo\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "#lista de listas\n",
    "list_of_list = []\n",
    "# recorro todas las imagenes de la carpeta\n",
    "for archivo in os.listdir(PATH_IMAGE):\n",
    "    #print(archivo)\n",
    "    L_rojo = []\n",
    "    L_verde = []\n",
    "    L_azul = []\n",
    "    \n",
    "    Name_Image = archivo #nombre del archivo\n",
    "    Barco = archivo[0] #nombre del archivo\n",
    "    \n",
    "    imagen = Image.open(PATH_IMAGE + archivo) # abro la imagen\n",
    "    pixeles = list(imagen.getdata()) # trasformo a pixeles rgb\n",
    "    \n",
    "    for pixel in pixeles:\n",
    "        rojo, verde, azul = pixel\n",
    "        L_rojo.append(rojo)\n",
    "        L_verde.append(verde)\n",
    "        L_azul.append(azul)\n",
    "\n",
    "    \n",
    "    L_pixeles = []\n",
    "    \n",
    "    L_pixeles.extend(L_rojo)\n",
    "    L_pixeles.extend(L_verde)\n",
    "    L_pixeles.extend(L_azul)\n",
    "    L_pixeles.append(Barco)\n",
    "    L_pixeles.append(Name_Image)\n",
    "    \n",
    "    list_of_list.append(L_pixeles)\n",
    "\n",
    "#Tranforma la lista a dataframe\n",
    "df = pd.DataFrame(list_of_list)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "print('Tiempo de ejecución ' + str(elapsed))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cambia el nombre de las columnas\n",
    "df.columns = DATASET_COLUMNS\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# genero el archivo\n",
    "df.to_pickle('Archivos_pkl/data_{}x{}.pkl'.format(PICTURE_SIZE, PICTURE_SIZE)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_barco = pd.read_pickle('Archivos_pkl/data_{}x{}.pkl'.format(PICTURE_SIZE, PICTURE_SIZE))\n",
    "data_barco.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Etiquetas_barco = list(data_barco.Barco.unique())\n",
    "Etiquetas_barco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barcos_imagenes(samples, title='file'):\n",
    "    for index, sample in samples.iterrows():\n",
    "        if title is not None:\n",
    "            if isinstance(title, str):\n",
    "                title = [title, ]\n",
    "            title_text = ', '.join(str(sample[title_field]) for title_field in title)\n",
    "            plt.title(title_text)\n",
    "\n",
    "        sample_as_grid = sample[INPUT_COLUMNS].values.reshape(len(CHANNELS), PICTURE_SIZE, PICTURE_SIZE).astype(np.float)\n",
    "        sample_as_grid = np.transpose(sample_as_grid, (1, 2, 0)) / 255\n",
    "        \n",
    "        plt.axis('off')\n",
    "        plt.imshow(sample_as_grid, interpolation='nearest')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcos_imagenes(data_barco.sample(5), 'Barco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, not_train = train_test_split(data_barco, test_size=0.4)\n",
    "validation, test = train_test_split(not_train, test_size=0.5)\n",
    "\n",
    "sets = (\n",
    "    ('train', train),\n",
    "    ('test', test),\n",
    ")\n",
    "\n",
    "print (' Train:', train.shape, '\\n', 'Validation:', validation.shape, '\\n', 'Test:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como quedan balanceado los datos\n",
    "f,ax=plt.subplots(1,3,figsize=(16,6))\n",
    "\n",
    "ax[0].set_title('Train')\n",
    "train.Barco.value_counts().plot.pie(autopct='%1.0f%%', figsize=(8,8), ax=ax[0])\n",
    "\n",
    "ax[1].set_title('Validation')\n",
    "validation.Barco.value_counts().plot.pie(autopct='%1.0f%%', figsize=(8,8), ax=ax[1])\n",
    "\n",
    "ax[2].set_title('Test')\n",
    "test.Barco.value_counts().plot.pie(autopct='%1.0f%%', figsize=(8,8), ax=ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_inputs(dataset):\n",
    "    return dataset[INPUT_COLUMNS].values / 255\n",
    "   \n",
    "def extract_outputs(dataset):\n",
    "    is_barco_columns = [(dataset.Barco == barco).values for barco in Etiquetas_barco]\n",
    "    return np.array(is_barco_columns).T\n",
    "\n",
    "def add_predictions(dataset, model):\n",
    "    predictions = model.predict(extract_inputs(dataset))\n",
    "\n",
    "    for numero_barco, barco in enumerate(Etiquetas_barco):\n",
    "        dataset[barco] = predictions[:, numero_barco]\n",
    "    \n",
    "    dataset['prediction'] = dataset[Etiquetas_barco].idxmax(axis=1)\n",
    "    dataset['prediction_confidence'] = dataset[Etiquetas_barco].max(axis=1)\n",
    "    dataset['correct'] = dataset.prediction == dataset.Barco\n",
    "    \n",
    "def show_predictions(model):\n",
    "    for set_name, set_data in sets:\n",
    "        add_predictions(set_data, model)\n",
    "\n",
    "        print('#' * 25, set_name, '#' * 25)\n",
    "        print('accuracy', accuracy_score(set_data.Barco, set_data.prediction))\n",
    "\n",
    "        plt.figure(figsize=(3,4))\n",
    "\n",
    "        plt.xticks([0, 1, 2, 3], Etiquetas_barco, rotation=45)\n",
    "        plt.yticks([0, 1, 2, 3], Etiquetas_barco)\n",
    "        plt.xlabel('Predicted class')\n",
    "        plt.ylabel('True class')\n",
    "\n",
    "        plt.title(set_name)\n",
    "\n",
    "        plt.imshow(\n",
    "            confusion_matrix(set_data.Barco, set_data.prediction), \n",
    "            cmap=plt.cm.Blues,\n",
    "            interpolation='nearest',\n",
    "        )\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea el modelo de Red Neuronal MLP\n",
    "MLPModel = MLPClassifier(activation='relu', solver='adam', hidden_layer_sizes=(50), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyMLP = MLPModel.fit(extract_inputs(train), extract_outputs(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(MLPModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusiones: es un modelo que da muy buenos resultados, para el poco tiempo que nos demando el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConvolution = Sequential([\n",
    "    Convolution2D(8, kernel_size=(4, 4), strides=(1, 1),\n",
    "                  activation='relu', \n",
    "                  input_shape=(PICTURE_SIZE, PICTURE_SIZE, len(CHANNELS)),)\n",
    "])\n",
    "\n",
    "modelConvolution.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyConvolution = modelConvolution.fit(\n",
    "    train[INPUT_COLUMNS].values.reshape(len(train), PICTURE_SIZE, PICTURE_SIZE, len(CHANNELS)) / 255,\n",
    "    extract_outputs(train),\n",
    "    batch_size=128,\n",
    "    epochs=5,\n",
    "    validation_data=(\n",
    "        test[INPUT_COLUMNS].values.reshape(len(test), PICTURE_SIZE, PICTURE_SIZE, len(CHANNELS)) / 255,\n",
    "        extract_outputs(test),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(modelConvolution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
